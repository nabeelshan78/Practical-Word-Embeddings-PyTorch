{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a word2vec model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import Dataset\n",
    "from gensim.models import Word2Vec\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.vocab import vocab\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_words(target_word, embedding_dict, top_k=2):\n",
    "    if target_word not in embedding_dict:\n",
    "        return f\"Word '{target_word}' not found in embeddings.\"\n",
    "    \n",
    "    target_vector = embedding_dict[target_word]\n",
    "    similarities = {}\n",
    "\n",
    "    for word, vector in embedding_dict.items():\n",
    "        if word == target_word:\n",
    "            continue\n",
    "        similarity = np.dot(target_vector, vector) / (np.linalg.norm(target_vector) * np.linalg.norm(vector))\n",
    "        similarities[word] = similarity\n",
    "\n",
    "    sorted_words = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "    return [word for word, _ in sorted_words[:top_k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a word2vec model from gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i', 'like', 'to', 'eat', 'pizza'],\n",
       " ['pizza', 'is', 'my', 'favorite', 'food'],\n",
       " ['i', 'enjoy', 'eating', 'pasta'],\n",
       " ['pasta', 'is', 'also', 'delicious', 'and', 'nutritious'],\n",
       " ['my', 'friends', 'and', 'i', 'often', 'order', 'pizza', 'for', 'dinner'],\n",
       " ['healthy', 'food', 'choices', 'include', 'vegetables', 'and', 'fruits'],\n",
       " ['sometimes', 'i', 'like', 'to', 'cook', 'italian', 'recipes'],\n",
       " ['eating',\n",
       "  'out',\n",
       "  'can',\n",
       "  'be',\n",
       "  'fun',\n",
       "  'but',\n",
       "  'home-cooked',\n",
       "  'meals',\n",
       "  'are',\n",
       "  'better'],\n",
       " ['do', 'you', 'prefer', 'pizza', 'or', 'pasta', '?'],\n",
       " ['i', 'always', 'try', 'new', 'recipes', 'on', 'weekends']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\n",
    "    [\"I\", \"like\", \"to\", \"eat\", \"pizza\"],\n",
    "    [\"Pizza\", \"is\", \"my\", \"favorite\", \"food\"],\n",
    "    [\"I\", \"enjoy\", \"eating\", \"pasta\"],\n",
    "    [\"Pasta\", \"is\", \"also\", \"delicious\", \"and\", \"nutritious\"],\n",
    "    [\"My\", \"friends\", \"and\", \"I\", \"often\", \"order\", \"pizza\", \"for\", \"dinner\"],\n",
    "    [\"Healthy\", \"food\", \"choices\", \"include\", \"vegetables\", \"and\", \"fruits\"],\n",
    "    [\"Sometimes\", \"I\", \"like\", \"to\", \"cook\", \"Italian\", \"recipes\"],\n",
    "    [\"Eating\", \"out\", \"can\", \"be\", \"fun\", \"but\", \"home-cooked\", \"meals\", \"are\", \"better\"],\n",
    "    [\"Do\", \"you\", \"prefer\", \"pizza\", \"or\", \"pasta\", \"?\"],\n",
    "    [\"I\", \"always\", \"try\", \"new\", \"recipes\", \"on\", \"weekends\"]\n",
    "]\n",
    "\n",
    "# Convert all words to lowercase for consistency\n",
    "sentences = [[word.lower() for word in sentence] for sentence in sentences]\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(\n",
    "    sentences,       # our training data: a list of tokenized sentences\n",
    "    vector_size=100, # Embedding dimension for each word (100-D vectors)\n",
    "    window=3,        # Context window size (3 words to the left & right)\n",
    "    min_count=1,     # Ignore words that appear less than 1 time\n",
    "    workers=4        # Number of CPU cores to use in training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.build_vocab(sentences, progress_per=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(199, 670)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model on training data\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=10, report_delay=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have trained a word2vec model using the `gensim` library.we can now access the word embeddings using `model.wv` and explore various operations such as finding similar words, calculating word similarities, and more.    \n",
    "\n",
    "Use the trained model to find similar words to \"pizza\" and calculate the similarity between \"pizza\" and \"pasta\". \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words to 'pizza': [('delicious', 0.2527044713497162), ('sometimes', 0.20118068158626556), ('better', 0.1951061487197876), ('out', 0.17653489112854004), ('to', 0.17132723331451416), ('also', 0.15045233070850372), ('are', 0.14713196456432343), ('is', 0.1391201913356781), ('friends', 0.11050225049257278), ('healthy', 0.09928988665342331)]\n"
     ]
    }
   ],
   "source": [
    "# Finding similar words\n",
    "similar_words = w2v_model.wv.most_similar(\"pizza\")\n",
    "print(\"Similar words to 'pizza':\", similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words to 'eating': [('?', 0.2684538662433624), ('home-cooked', 0.14282964169979095), ('also', 0.1284063458442688), ('weekends', 0.10976455360651016), ('pasta', 0.10961795598268509), ('favorite', 0.10887644439935684), ('include', 0.10816717147827148), ('vegetables', 0.10177747160196304), ('choices', 0.09926041215658188), ('often', 0.09616386890411377)]\n"
     ]
    }
   ],
   "source": [
    "similar_words = w2v_model.wv.most_similar(\"eating\")\n",
    "print(\"Similar words to 'eating':\", similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'pizza' and 'pasta': -0.005449652\n"
     ]
    }
   ],
   "source": [
    "# Calculating word similarity\n",
    "similarity = w2v_model.wv.similarity(\"pizza\", \"pasta\")\n",
    "print(\"Similarity between 'pizza' and 'pasta':\", similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'pizza' and 'delicious': 0.25270447\n"
     ]
    }
   ],
   "source": [
    "similarity = w2v_model.wv.similarity(\"pizza\", \"delicious\")\n",
    "print(\"Similarity between 'pizza' and 'delicious':\", similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word embeddings obtained from the model would be more meaningful and informative with larger and more diverse training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the trained model to create a PyTorch embedding layer and use it in any task as an embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'pizza', 'and', 'pasta', 'is', 'my', 'food', 'eating', 'to', 'like', 'recipes', 'eat', 'often', 'healthy', 'dinner', 'for', 'order', 'delicious', 'friends', 'nutritious', 'also', 'include', 'enjoy', 'favorite', 'choices', 'weekends', 'on', 'are', 'new', 'try', 'always', '?', 'or', 'prefer', 'you', 'do', 'better', 'meals', 'fruits', 'home-cooked', 'but', 'fun', 'be', 'can', 'out', 'italian', 'cook', 'sometimes', 'vegetables']\n",
      "[[-0.0005448   0.00021663  0.00507805 ... -0.00705724  0.00088444\n",
      "   0.0063876 ]\n",
      " [-0.00714774  0.00125787 -0.00719377 ...  0.00485981  0.00078154\n",
      "   0.00302692]\n",
      " [ 0.00768154  0.00913524  0.00111634 ...  0.00830145 -0.00610254\n",
      "   0.00946844]\n",
      " ...\n",
      " [ 0.00210629  0.00573915 -0.00212659 ...  0.00444153 -0.00810177\n",
      "  -0.00406681]\n",
      " [-0.00696055 -0.00245817 -0.00802504 ...  0.00274619  0.00561105\n",
      "   0.00257519]\n",
      " [-0.00497406 -0.00127645  0.00327384 ... -0.00696506  0.00576213\n",
      "  -0.0094267 ]]\n"
     ]
    }
   ],
   "source": [
    "print(w2v_model.wv.index_to_key)\n",
    "print(w2v_model.wv.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "print(len(w2v_model.wv.index_to_key))\n",
    "print(len(w2v_model.wv.vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0005,  0.0002,  0.0051,  ..., -0.0071,  0.0009,  0.0064],\n",
       "        [-0.0071,  0.0013, -0.0072,  ...,  0.0049,  0.0008,  0.0030],\n",
       "        [ 0.0077,  0.0091,  0.0011,  ...,  0.0083, -0.0061,  0.0095],\n",
       "        ...,\n",
       "        [ 0.0021,  0.0057, -0.0021,  ...,  0.0044, -0.0081, -0.0041],\n",
       "        [-0.0070, -0.0025, -0.0080,  ...,  0.0027,  0.0056,  0.0026],\n",
       "        [-0.0050, -0.0013,  0.0033,  ..., -0.0070,  0.0058, -0.0094]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors = w2v_model.wv\n",
    "word_to_index = {word: index for index, word in enumerate(word_vectors.index_to_key)}\n",
    "\n",
    "# Create an instance of nn.Embedding and load it with the trained vectors\n",
    "embedding_dim = w2v_model.vector_size\n",
    "embedding = torch.nn.Embedding(len(word_vectors.index_to_key), embedding_dim)\n",
    "embedding.weight.data.copy_(torch.from_numpy(word_vectors.vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: pizza, Embedding: [[-0.00714774  0.00125787 -0.00719377 -0.002252    0.0037566   0.00583821\n",
      "   0.00122404  0.00211798 -0.00412855  0.00723212 -0.00633135  0.00459581\n",
      "  -0.00820093  0.00205702 -0.00499275 -0.00427009 -0.00306797  0.0056457\n",
      "   0.00580719 -0.00504387  0.00079023 -0.00848691  0.00781246  0.00922173\n",
      "  -0.00274401  0.00082376  0.00072107  0.00548828 -0.00863547  0.00056937\n",
      "   0.00687279  0.00222707  0.00113017 -0.00939239  0.00846144 -0.00623696\n",
      "  -0.00297992  0.00349776 -0.00079536  0.0014085   0.00178815 -0.00684685\n",
      "  -0.00971646  0.00909191  0.00624171 -0.00692963  0.00335529  0.00018195\n",
      "   0.00478414 -0.00712823  0.00402592  0.00435332  0.00997025 -0.00449254\n",
      "  -0.00139026 -0.00733258 -0.00967484 -0.0090679  -0.00100836 -0.00650289\n",
      "   0.00486133 -0.00623833  0.00254642  0.00078534 -0.00338036 -0.00094708\n",
      "   0.00994053  0.00917958 -0.00450207  0.00911223 -0.0056666   0.00595734\n",
      "  -0.00305531  0.00343974  0.00304454  0.00688019 -0.00234786  0.00879942\n",
      "   0.00760095 -0.00956126 -0.00809464 -0.00766791  0.00288259 -0.00279794\n",
      "  -0.00694146 -0.00813824  0.008354    0.00200748 -0.00932193 -0.00480621\n",
      "   0.00316409 -0.00468091  0.00529693 -0.00421008  0.00267268 -0.0080381\n",
      "   0.00620313  0.00485981  0.00078154  0.00302692]]\n"
     ]
    }
   ],
   "source": [
    "# get the embedding for a word\n",
    "word = \"pizza\"\n",
    "word_index = word_to_index[word]\n",
    "word_embedding = embedding(torch.LongTensor([word_index]))\n",
    "print(f\"Word: {word}, Embedding: {word_embedding.detach().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
